# Data Sharing basics

- Data is primarily populated by data locked in silos.
- Modern data sharing is the age where data silos are being broken down into a centralized single source of truth across and beyond the enterprise.
- The organization that shares its data is the data provider. The one that uses the shared data is the data consumer.

Broadly speaking, three types of data form the foundation for business operations and analytics within most enterprises:

- First-party data is data you collect directly — for instance, from
  interactions with customers and prospects.
- Second-party data is produced by or in collaboration with trusted
  partners, such as data from a software-as-a-service (SaaS) vendor.
- Third-party data is data acquired from external sources that don’t
  have a relationship with your organization. Common examples
  include demographic, weather, and financial market data.

Traditional data sharing methods are riddled with problems, making discovering, capturing, and using all this data a challenge.

- Email: A data file is emailed from a provider to a consumer.
- File transfer: Data files are shared and downloaded
  between two computers or via the Internet through File
  Transfer Protocol (FTP).
- Application programming interfaces (APIs): A proprietary
  API is used to initiate and manage the data transfer.
- Extract, transform, load (ETL) software: ETL software
  extracts data from the provider’s database, transforms it into
  a format suitable for consumption, and then loads it into the
  consumer’s database.
- Cloud storage: The provider stores a copy of the data and provides the consumer with credentials for accessing it.

With modern data sharing technology, a data provider can easily grant governed access to the data it wants to share with its
intended consumers without managing cumbersome data pipelines. End-to-end security, multiparty governance, and metadata
management services are systematically applied, even when the
data consumers span multiple clouds. With updates made automatically, you don’t have to link applications, set up file-sharing
procedures, or frequently upload new data to keep data current.

# Data sharing challenges

Traditional options for sharing data at scale require provisioning and scaling complex computing platforms, even for small
slices of data. Managing the associated technology infrastructure
places an administrative burden on IT teams. Ongoing maintenance consumes valuable time and expertise, sapping scarce IT
resources and diverting attention from other projects.

- Handling increased data size
- Maintaining data pipelines

The accumulation of these steps results in slow and painful processes for data providers and consumers. All of this must happen
before attempting to analyze and develop insights from the data,
which delays time to value.

- Growth is limited because critical business decisions are
  made based on outdated, incomplete, or inaccurate data.
- Limited discovery of third-party data and services and data
  locked in silos limits your capability to create a 360-degree
  view of customers and unlock growth opportunities.
- Building and maintaining multiple and disparate data
  sharing tools results in increased costs.
- Working with data across different silos decreases the
  efficiency of both technical and business users.
  » With data moving across environments, securely governing
  access and following regulatory and compliance requirements is nearly impossible.
- The risk of a data breach or accidental data loss/disclosure
  multiplies, along with their associated costs, such as breach
  notifications, credit monitoring services, damage to an
  organization’s brand, customer churn, litigation, forensic
  analysis, and recovery

# Business Value of Sharing Data
